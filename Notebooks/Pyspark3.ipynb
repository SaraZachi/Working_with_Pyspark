{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwWgr8oAn1F8ySTfWMAfGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraZachi/Working_with_Pyspark/blob/main/Notebooks/Pyspark3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy1PDppItojs",
        "outputId": "e0b6adf1-d51c-4231-da6c-cd62a6fe76dc"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRaCDkbpuJo4"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "081y6WyEuKzA",
        "outputId": "1e3fdd64-c080-4dad-e625-3f51c3a1fbb5"
      },
      "source": [
        "# Starting Pyspark session => setting up\n",
        "spark=SparkSession.builder.appName('DataFrame').getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ebdb8db8128e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataFrame</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f163a74c2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFqzOxeiuMJS",
        "outputId": "0b1a87c1-e55e-4431-f7eb-a3d8404d8f05"
      },
      "source": [
        "df_pyspark = spark.read.option('delimiter', ';').csv('/content/test3.csv', header=True, inferSchema=True)\n",
        "df_pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Name: string, Age: int, Experience: int, Salary: int]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09ZhrQRYuOvV",
        "outputId": "328805d8-b652-431c-fbd0-132e31677722"
      },
      "source": [
        "df_pyspark.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+----------+------+\n",
            "| Name| Age|Experience|Salary|\n",
            "+-----+----+----------+------+\n",
            "|Chris|  30|        10| 30000|\n",
            "|  Sam|  31|         8| 25000|\n",
            "|  Dan|  29|         4| 20000|\n",
            "|  Ana|  24|         3| 20000|\n",
            "| Cloe|  21|         1| 15000|\n",
            "| Carl|  23|         2| 18000|\n",
            "|Piper|null|      null| 40000|\n",
            "| null|  34|        10| 38000|\n",
            "| null|  36|      null|  null|\n",
            "+-----+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV7jgYEquQy2",
        "outputId": "211b2e15-55a9-444f-f923-d1fcbd7aa421"
      },
      "source": [
        "# Dropp column\n",
        "df_pyspark.drop('Name').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+\n",
            "| Age|Experience|Salary|\n",
            "+----+----------+------+\n",
            "|  30|        10| 30000|\n",
            "|  31|         8| 25000|\n",
            "|  29|         4| 20000|\n",
            "|  24|         3| 20000|\n",
            "|  21|         1| 15000|\n",
            "|  23|         2| 18000|\n",
            "|null|      null| 40000|\n",
            "|  34|        10| 38000|\n",
            "|  36|      null|  null|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4yD2lbWuS_F",
        "outputId": "80fadc2f-2797-4473-e55a-4a26f62ffddc"
      },
      "source": [
        "df_pyspark.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+----------+------+\n",
            "| Name| Age|Experience|Salary|\n",
            "+-----+----+----------+------+\n",
            "|Chris|  30|        10| 30000|\n",
            "|  Sam|  31|         8| 25000|\n",
            "|  Dan|  29|         4| 20000|\n",
            "|  Ana|  24|         3| 20000|\n",
            "| Cloe|  21|         1| 15000|\n",
            "| Carl|  23|         2| 18000|\n",
            "|Piper|null|      null| 40000|\n",
            "| null|  34|        10| 38000|\n",
            "| null|  36|      null|  null|\n",
            "+-----+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRYt4a1_uUqt",
        "outputId": "d446f356-0653-4893-d1f6-01af3014fb12"
      },
      "source": [
        "# Drop entire row with null values by default\n",
        "df_pyspark.na.drop().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+----------+------+\n",
            "| Name|Age|Experience|Salary|\n",
            "+-----+---+----------+------+\n",
            "|Chris| 30|        10| 30000|\n",
            "|  Sam| 31|         8| 25000|\n",
            "|  Dan| 29|         4| 20000|\n",
            "|  Ana| 24|         3| 20000|\n",
            "| Cloe| 21|         1| 15000|\n",
            "| Carl| 23|         2| 18000|\n",
            "+-----+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Yw7rEVuV0F",
        "outputId": "97deb6f1-ef40-4ce7-8aad-fe687a73f74c"
      },
      "source": [
        "# Any == how\n",
        "## Drop entire rows with any null value\n",
        "df_pyspark.na.drop(how='any').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+----------+------+\n",
            "| Name|Age|Experience|Salary|\n",
            "+-----+---+----------+------+\n",
            "|Chris| 30|        10| 30000|\n",
            "|  Sam| 31|         8| 25000|\n",
            "|  Dan| 29|         4| 20000|\n",
            "|  Ana| 24|         3| 20000|\n",
            "| Cloe| 21|         1| 15000|\n",
            "| Carl| 23|         2| 18000|\n",
            "+-----+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jADmYeHguW29",
        "outputId": "6ba477f9-f941-4b91-98d1-2a97c7db88ae"
      },
      "source": [
        "# Threshold\n",
        "## Deleting rows with more then a certain amount of not null cells\n",
        "### The 'thresh' comand will define how many not null cells we need to have.\n",
        "### If there are less non null cells then the threshold variable, the row is deleted.\n",
        "df_pyspark.na.drop(how='any', thresh=3).show()\n",
        "df_pyspark.na.drop(how='any', thresh=2).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+----------+------+\n",
            "| Name|Age|Experience|Salary|\n",
            "+-----+---+----------+------+\n",
            "|Chris| 30|        10| 30000|\n",
            "|  Sam| 31|         8| 25000|\n",
            "|  Dan| 29|         4| 20000|\n",
            "|  Ana| 24|         3| 20000|\n",
            "| Cloe| 21|         1| 15000|\n",
            "| Carl| 23|         2| 18000|\n",
            "| null| 34|        10| 38000|\n",
            "+-----+---+----------+------+\n",
            "\n",
            "+-----+----+----------+------+\n",
            "| Name| Age|Experience|Salary|\n",
            "+-----+----+----------+------+\n",
            "|Chris|  30|        10| 30000|\n",
            "|  Sam|  31|         8| 25000|\n",
            "|  Dan|  29|         4| 20000|\n",
            "|  Ana|  24|         3| 20000|\n",
            "| Cloe|  21|         1| 15000|\n",
            "| Carl|  23|         2| 18000|\n",
            "|Piper|null|      null| 40000|\n",
            "| null|  34|        10| 38000|\n",
            "+-----+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSDDrAaewXhP",
        "outputId": "f5c96d3c-424f-4542-e0b0-1f713cbbb3dc"
      },
      "source": [
        "# Subset\n",
        "## Drop rows with null values in a specific column\n",
        "df_pyspark.na.drop(how='any', subset=['Experience']).show()\n",
        "df_pyspark.na.drop(how='any', subset=['Name']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+----------+------+\n",
            "| Name|Age|Experience|Salary|\n",
            "+-----+---+----------+------+\n",
            "|Chris| 30|        10| 30000|\n",
            "|  Sam| 31|         8| 25000|\n",
            "|  Dan| 29|         4| 20000|\n",
            "|  Ana| 24|         3| 20000|\n",
            "| Cloe| 21|         1| 15000|\n",
            "| Carl| 23|         2| 18000|\n",
            "| null| 34|        10| 38000|\n",
            "+-----+---+----------+------+\n",
            "\n",
            "+-----+----+----------+------+\n",
            "| Name| Age|Experience|Salary|\n",
            "+-----+----+----------+------+\n",
            "|Chris|  30|        10| 30000|\n",
            "|  Sam|  31|         8| 25000|\n",
            "|  Dan|  29|         4| 20000|\n",
            "|  Ana|  24|         3| 20000|\n",
            "| Cloe|  21|         1| 15000|\n",
            "| Carl|  23|         2| 18000|\n",
            "|Piper|null|      null| 40000|\n",
            "+-----+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy0p1xqG9phW",
        "outputId": "b861237b-3830-4cc3-c449-c53ae0d48fe5"
      },
      "source": [
        "# Fill missing values\n",
        "## Obs.: you have to specify which column will be overwritten. By default, all the columns will be overwritten.\n",
        "##Additionally, you can't fill a column with strings with integer and vice versa.\n",
        "df_pyspark.na.fill('Missing Values') \\\n",
        "    .na.fill(0).show()\n",
        "# another way to wright: df_pyspark.na.fill(value=0,subset=['Age']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---+----------+------+\n",
            "|          Name|Age|Experience|Salary|\n",
            "+--------------+---+----------+------+\n",
            "|         Chris| 30|        10| 30000|\n",
            "|           Sam| 31|         8| 25000|\n",
            "|           Dan| 29|         4| 20000|\n",
            "|           Ana| 24|         3| 20000|\n",
            "|          Cloe| 21|         1| 15000|\n",
            "|          Carl| 23|         2| 18000|\n",
            "|         Piper|  0|         0| 40000|\n",
            "|Missing Values| 34|        10| 38000|\n",
            "|Missing Values| 36|         0|     0|\n",
            "+--------------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k23lsHax-Vv9",
        "outputId": "5df3270b-4c46-4bf7-c51c-080a7b3a3a28"
      },
      "source": [
        "df_pyspark.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+----------+------+\n",
            "| Name| Age|Experience|Salary|\n",
            "+-----+----+----------+------+\n",
            "|Chris|  30|        10| 30000|\n",
            "|  Sam|  31|         8| 25000|\n",
            "|  Dan|  29|         4| 20000|\n",
            "|  Ana|  24|         3| 20000|\n",
            "| Cloe|  21|         1| 15000|\n",
            "| Carl|  23|         2| 18000|\n",
            "|Piper|null|      null| 40000|\n",
            "| null|  34|        10| 38000|\n",
            "| null|  36|      null|  null|\n",
            "+-----+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcB5cqovIccX"
      },
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=['Age', 'Experience', 'Salary'],\n",
        "    outputCols=['{}_imputed'.format(c) for c in ['Age', 'Experience', 'Salary']]\n",
        "    ).setStrategy('mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mXiL3tJJR9C",
        "outputId": "b65ebb2b-97de-4f0a-97c4-22813601aea5"
      },
      "source": [
        "#  Add imputation cols to df\n",
        "imputer.fit(df_pyspark).transform(df_pyspark).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+----------+------+-----------+------------------+--------------+\n",
            "| Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
            "+-----+----+----------+------+-----------+------------------+--------------+\n",
            "|Chris|  30|        10| 30000|         30|                10|         30000|\n",
            "|  Sam|  31|         8| 25000|         31|                 8|         25000|\n",
            "|  Dan|  29|         4| 20000|         29|                 4|         20000|\n",
            "|  Ana|  24|         3| 20000|         24|                 3|         20000|\n",
            "| Cloe|  21|         1| 15000|         21|                 1|         15000|\n",
            "| Carl|  23|         2| 18000|         23|                 2|         18000|\n",
            "|Piper|null|      null| 40000|         28|                 5|         40000|\n",
            "| null|  34|        10| 38000|         34|                10|         38000|\n",
            "| null|  36|      null|  null|         36|                 5|         25750|\n",
            "+-----+----+----------+------+-----------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvmlqLsQJwvx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}